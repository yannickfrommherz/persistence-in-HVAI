{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e139bf92-1b13-41e9-88cb-a6857971520d",
   "metadata": {},
   "source": [
    "# VACC\n",
    "\n",
    "In this notebook all preprocessing steps for creating the final VACC corpus are run as well as the persistence tagging algorithm for the qualitative analysis. \n",
    "\n",
    "As mentioned, the actual data used in the doctoral thesis needs to be requested from Ingo Siegert.\n",
    "\n",
    "However, a dummy dataset is provided for the code to be executable. This fictitious dataset has the exact same structure as the actual data, but content-wise it is highly repetitive such that all four interactions of all four dummy participants contain the same turns. \n",
    "\n",
    "Once the actual data is available, replace the dummy dataset with it, maintaining folder structure.\n",
    "\n",
    "Refer to the relevant chapters in the doctoral thesis for further explanation of the steps below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfd246-29f5-40ae-b866-af800e046123",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19d43af6-ba5b-41e6-8967-fb91b38db8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading modules.\n"
     ]
    }
   ],
   "source": [
    "#importing relevant modules\n",
    "\n",
    "import pandas as pd, json, sys, os, warnings, re\n",
    "from importlib import reload\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "sys.path.append(\"../Code/\")\n",
    "\n",
    "if \"run_before\" not in locals():\n",
    "    print(\"Importing modules for the first time.\")\n",
    "    import preprocessing, persistence, visualisation\n",
    "    run_before = \"yes\"\n",
    "else:\n",
    "    print(\"Reloading modules.\")\n",
    "    reload(preprocessing)\n",
    "    reload(persistence)\n",
    "    reload(visualisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b3f7202-1393-4f7f-8c32-8ae0a4f6d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining corpus name \n",
    "#(some modules contain corpora-specific code, thus a variable with the corpus name is needed to ensure the right code is executed)\n",
    "which_corpus = \"VACC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4224c2-345c-4df8-9984-cfd16f875bf5",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Creating one csv file\n",
    "\n",
    "The following code creates *one* csv file containing all turns from all interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dcd29a5-f672-47f8-90fa-19deb0a545a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to folders with transcripts and speaker list (who made which utterance)\n",
    "root_transcripts = f\"1_Corpus/Original_files/Fertige_Transkripte/\"\n",
    "root_speakers = f\"1_Corpus/Original_files/Fertige_Utterances/\"\n",
    "output_destination = f\"1_Corpus/Corpus_{which_corpus}.csv\"\n",
    "\n",
    "preprocessing.file_creator_vacc(root_transcripts, root_speakers, output_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268683eb-fbb0-4bdb-bbe8-2410a8adf45e",
   "metadata": {},
   "source": [
    "### Merging same-speaker turns\n",
    "\n",
    "The following code merges consecutive same-speaker turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb4955f-0bbd-4303-a298-60f5a1ff5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.turn_merger(f\"1_Corpus/Corpus_{which_corpus}.csv\", \n",
    "                          f\"1_Corpus/Corpus_merged_turns_{which_corpus}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd85f0-6aa7-4319-aa30-62808f2dc5f5",
   "metadata": {},
   "source": [
    "Note that, as explained in the thesis, manual unmerging was performed in certain cases. Hence, when running this notebook with the actual data, it will eventually differ slightly from the data used in the study as this manual step as well as the manual lemma correction step mentioned below cannot be replicated here.\n",
    "\n",
    "### Tokenising and lemmatising data \n",
    "\n",
    "Note that, as explained in the thesis, POS-tagging was initially also performed, but as its output was not used, this step is disregarded here.\n",
    "\n",
    "Lemmatisation was performed with both TreeTagger and RNNTagger. As explained in the thesis, the RNNTagger's output proved to be most reliable. To execute the following steps you need to download the [RNNTagger](https://www.cis.lmu.de/~schmid/tools/RNNTagger/) and follow the installation steps indicated there. \n",
    "\n",
    "The RNNTagger expects a simple file with one token per row. To be able to remap tagged tokens to their corresponding metadata (which turn they belong to, by which speaker etc.), during tokenisation below, an additional list (`tokens_for_remapping`) is outputted which contains the same tokens as well as a turn boundary marker. This marker enables remapping the tokens to their respective turn, once they have been tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1474743-eccb-41be-af60-ecf87db77aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising using custom code for later token remapping\n",
    "tokens_for_remapping = preprocessing.tokenise(f\"1_Corpus/Corpus_merged_turns_{which_corpus}.csv\", \n",
    "                                              \"2_Preprocessed/Files/txt_file_for_tagger.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c86358-c4c1-47f5-a7c4-c8b205416ffa",
   "metadata": {},
   "source": [
    "Next, run the following lines in your command line, replacing \"rnntagger_path\" with the absolute path to your RNNTagger directory, \"txt_file_for_tagger.txt\" with the absolute path to said file (created in the cell above) and \"output\" with the absolute path leading to a new file called \"VACC/2_Preprocessed/Files/RNN_tagged.txt\" (i.e., complement that path based on where you stored the entire repository on your computer). Execute the middle line only if permission errors occur."
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb2e6ea8-a447-4f2e-ba4b-b6d53c33b0fe",
   "metadata": {},
   "source": [
    "cd rnntagger_path\n",
    "chmod -R +x . \n",
    "cmd/rnn-tagger-german.sh txt_file_for_tagger.txt > VACC/2_Preprocessed/Files/RNN_tagged.txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8729c47a-d746-4f31-8356-392d3ff4e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remapping tagged tokens to their respective turn\n",
    "preprocessing.remap(f\"1_Corpus/Corpus_merged_turns_{which_corpus}.csv\", #corpus\n",
    "                    f\"2_Preprocessed/Files/RNN_tagged.txt\", #tagger output\n",
    "                    tokens_for_remapping, #needed for remapping tagger output to corpus\n",
    "                    f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", #destination of remapped corpus\n",
    "                    which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef32d7-3610-4f9d-b4e2-694062768559",
   "metadata": {},
   "source": [
    "Note that, as explained in the thesis, manual lemma correction was performed at this point. \n",
    "\n",
    "### Creating lemma bi-, tri-, and quadrigrams\n",
    "\n",
    "The following code creates lemma n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a42d96fc-fb9e-410f-88e5-295dde47a6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigrams\n",
      "trigrams\n",
      "quadrigrams\n"
     ]
    }
   ],
   "source": [
    "preprocessing.ngrammer(f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414814c4-ac0c-4bf2-a8aa-30aebe2bc169",
   "metadata": {},
   "source": [
    "Preprocessing of the corpus is now done. Note again that even if you ran the code with the actual data, it will be slightly different from the data used in the thesis due to non-replicable manual steps (see above). \n",
    "\n",
    "For the quantitative analysis see separate notebooks in the directory \"Quantitative_Analysis\". This code is in separate notebooks as some quantitative analyses extend beyond one single corpus and are thus organised with regard to alternation sets. Also, a different programming language (R) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed2dd3-5400-4ece-bdf6-91804d6df5b6",
   "metadata": {},
   "source": [
    "## Persistence Tagging for the Qualitative Analysis\n",
    "\n",
    "As mentioned in the thesis, the persistence tagging algorithm can be used to tag cases of persistence on multiple levels such as lemmata, POS-tags etc. However, the qualitative analysis in the thesis relied solely on lemma-based tagging. This is defined in the following cell, along with stop lemmata and lemmata from the instructions, both of which will be excluded from tagging. For simplicity, the actual instructions are excluded even when using dummy data.\n",
    "\n",
    "Also note that `tagger` can be used to tag, e.g., cases of persistence from the human speaker to the voice assistant (i.e., tagging cases of quasi-persistence). For that, pass different values for the corresponding arguments than the default ones which implement allo-persistence from voice assistant to human speaker. \n",
    "\n",
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36e99bce-074a-4bdb-9c3a-43f5c0603b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\"lemma\"] #defining level to tag on \n",
    "\n",
    "stoplemmas = ['an', 'der', 'ein', 'es', 'für', 'haben', 'ich', 'in', 'mit', \n",
    "              'nicht', 'oder', 'sein', 'um', 'und', 'von', 'werden', 'zu'] #defining stopwords to exclude from tagging\n",
    "\n",
    "#reading and defining instructions for Quiz interactions w/o confederate as well as Calendar interactions\n",
    "with open(\"Instructions/Lemmata_in_instructions_with_confederate.txt\") as f, open(\"Instructions/Lemmata_in_instructions_without_confederate.txt\") as g, open(\"Instructions/Lemmata_in_visual_schedule.txt\") as h:\n",
    "    instructions_with_jannik, instructions_without_jannik, schedule = f.read().split(), g.read().split(), h.read().split()\n",
    "\n",
    "instructions = [instructions_without_jannik, instructions_with_jannik, schedule] #creating list of instructions to pass below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10bacb-d97e-41cf-9708-e0f95e364c7a",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "\n",
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd7f9932-b857-4416-9cf5-008b271318b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 16/16 [00:00<00:00, 69.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent SPP's on lemma level: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#reading unigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing unigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_unigrams.csv\", instructions, stoplemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bcd00-1a52-4d04-b302-9bf7094177dd",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37b295f8-bf01-4595-8f02-b67b4b001c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 16/16 [00:00<00:00, 62.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent SPP's on lemma level: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#creating bigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_bigrams = [[[\" \".join(instructions_without_jannik[i:i+2]) for i in range(len(instructions_without_jannik)-2+1)]],\n",
    "                       [[\" \".join(instructions_with_jannik[i:i+2]) for i in range(len(instructions_with_jannik)-2+1)]], schedule]\n",
    "\n",
    "#reading bigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_bigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing bigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_bigrams.csv\", instructions, stoplemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767b52d-488b-48b5-bec2-5e03af9ac1cc",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "534e30c5-fe5b-4315-b8f0-55525a5ba8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 16/16 [00:00<00:00, 76.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent SPP's on lemma level: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#creating trigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_trigrams = [[[\" \".join(instructions_without_jannik[i:i+3]) for i in range(len(instructions_without_jannik)-3+1)]],\n",
    "                        [[\" \".join(instructions_with_jannik[i:i+3]) for i in range(len(instructions_with_jannik)-3+1)]], schedule]\n",
    "\n",
    "#reading trigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_trigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing trigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_trigrams.csv\", instructions, stoplemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c8c8c-9fca-4979-9e5f-65dfe687600f",
   "metadata": {},
   "source": [
    "#### Quadrigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6f3af43-dc62-4a60-b9ee-335c47642724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 16/16 [00:00<00:00, 89.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistent SPP's on lemma level: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#creating quadrigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_quadrigrams = [[[\" \".join(instructions_without_jannik[i:i+4]) for i in range(len(instructions_without_jannik)-4+1)]],\n",
    "                           [[\" \".join(instructions_with_jannik[i:i+4]) for i in range(len(instructions_with_jannik)-4+1)]], schedule]\n",
    "\n",
    "#reading quadrigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_quadrigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing quadrigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_quadrigrams.csv\", instructions, stoplemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82a4bc-5558-4b58-a51a-0d2068cdbc35",
   "metadata": {},
   "source": [
    "For simplicity, the following code combines all n-gram levels into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cc18dda-aefe-4686-82da-7f5b86f33ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 16/16 [00:00<00:00, 80.58it/s]\n"
     ]
    }
   ],
   "source": [
    "persistence.combiner(\"3_Persistence_tagged/single_ngrams\", \"3_Persistence_tagged\", which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610fd92-1730-494b-b741-f6b67b53587b",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "The following code visualises tagged cases of persistence on all n-grams levels in HTML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ab5d9ce-3b60-45bc-b29b-09b220329cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 16/16 [00:00<00:00, 39.66it/s]\n"
     ]
    }
   ],
   "source": [
    "path = \"3_Persistence_tagged\"\n",
    "\n",
    "visualisation.lemma(which_corpus, path, f\"{path}/visualisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbfbda-5499-41eb-8b24-b64e5ec73753",
   "metadata": {},
   "source": [
    "### Inspecting frequent cases of persistence\n",
    "\n",
    "The following code outputs frequent cases of persistence for each n-gram level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b8e069a-8329-433d-a0b6-04bd22e7552b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Frequent Persistent N-Grams in Speaker Turns\n",
      "\n",
      "Lemma Unigrams\n",
      "\n",
      "Termin                                        48\n",
      "Dezember                                      48\n",
      "mir                                           16\n",
      "Eintrag                                       16\n",
      "-------------------------------------------------\n",
      "\n",
      "Lemma Bigrams\n",
      "\n",
      "Termin an                                     48\n",
      "-------------------------------------------------\n",
      "\n",
      "No persistent lemma trigrams in speaker turns above 0\n",
      "\n",
      "No persistent lemma quadrigrams in speaker turns above 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visualisation.inspect(levels = [\"lemma\"], #further levels such as POS-tags could be supplied if tagging was performed on that level\n",
    "                      ngrams = [\"unigrams\", \"bigrams\", \"trigrams\", \"quadrigrams\"], \n",
    "                      threshold = 0, \n",
    "                      which_corpus =  which_corpus, \n",
    "                      path = \"3_Persistence_tagged/single_ngrams\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvai",
   "language": "python",
   "name": "hvai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
