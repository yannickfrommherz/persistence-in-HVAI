{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e139bf92-1b13-41e9-88cb-a6857971520d",
   "metadata": {},
   "source": [
    "# VACC\n",
    "\n",
    "In this notebook all preprocessing steps for creating the final VACC corpus are run as well as the persistence tagging algorithm for the qualitative analysis. \n",
    "\n",
    "As mentioned, the actual data used in the doctoral thesis needs to be requested from Ingo Siegert.\n",
    "\n",
    "However, a dummy dataset is provided for the code to be executable. This fictitious dataset has the exact same structure as the actual data, but content-wise it is highly repetitive such that all four interactions of all four dummy participants contain the same turns. \n",
    "\n",
    "Once the actual data is available, replace the dummy dataset with it, maintaining folder structure.\n",
    "\n",
    "Refer to the relevant chapters in the doctoral thesis for further explanation of the steps below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbfd246-29f5-40ae-b866-af800e046123",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d43af6-ba5b-41e6-8967-fb91b38db8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing relevant modules\n",
    "import pandas as pd, json, sys, os, warnings, re\n",
    "from tqdm import tqdm\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#informing Python about a custom code directory and importing some of the modules from there\n",
    "sys.path.append(\"../Code/\")\n",
    "import preprocessing, persistence, visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3f7202-1393-4f7f-8c32-8ae0a4f6d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining corpus name \n",
    "#(some modules contain corpora-specific code, thus a variable with the corpus name is needed to ensure the right code is executed)\n",
    "which_corpus = \"VACC\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4224c2-345c-4df8-9984-cfd16f875bf5",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "### Creating one csv file\n",
    "\n",
    "The following code creates *one* csv file containing all turns from all interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcd29a5-f672-47f8-90fa-19deb0a545a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paths to folders with transcripts and speaker list (who made which utterance)\n",
    "root_transcripts = f\"1_Corpus/Original_files/Fertige_Transkripte/\"\n",
    "root_speakers = f\"1_Corpus/Original_files/Fertige_Utterances/\"\n",
    "output_destination = f\"1_Corpus/Corpus_{which_corpus}.csv\"\n",
    "\n",
    "preprocessing.file_creator_vacc(root_transcripts, root_speakers, output_destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268683eb-fbb0-4bdb-bbe8-2410a8adf45e",
   "metadata": {},
   "source": [
    "### Merging same-speaker turns\n",
    "\n",
    "The following code merges consecutive same-speaker turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb4955f-0bbd-4303-a298-60f5a1ff5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.turn_merger(f\"1_Corpus/Corpus_{which_corpus}.csv\", \n",
    "                          f\"1_Corpus/Corpus_merged_turns_{which_corpus}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdd85f0-6aa7-4319-aa30-62808f2dc5f5",
   "metadata": {},
   "source": [
    "Note that, as explained in the thesis, manual unmerging was performed in certain cases. Hence, when running this notebook with the actual data, it will eventually differ from the data used in the study as this manual step as well as the manual lemma correction step mentioned below cannot be replicated here.\n",
    "\n",
    "### Tokenising and lemmatising data \n",
    "\n",
    "Note that, as explained in the thesis, POS-tagging was initially also performed. As its output was not used this step is disregarded here.\n",
    "\n",
    "Lemmatisation was performed with both TreeTagger and RNNTagger. As explained in the thesis, the RNNTagger's output proved to be most reliable. To execute the following steps you need to download the [RNNTagger](https://www.cis.lmu.de/~schmid/tools/RNNTagger/) and follow the installation steps indicated there. \n",
    "\n",
    "The RNNTagger expects a simple file with one token per row. To be able to remap tagged tokens to their corresponding metadata (which turn they belong to, by which speaker etc.), during tokenisation below, an additional list (`tokens_for_remapping`) is outputted which contains the same tokens as well as a turn boundary marker. This marker enables remapping the tokens to their respective turn, once they have been tagged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1474743-eccb-41be-af60-ecf87db77aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenising using custom code for later token remapping\n",
    "tokens_for_remapping = preprocessing.tokenise(f\"1_Corpus/Corpus_merged_turns_{which_corpus}.csv\", \n",
    "                                              \"2_Preprocessed/Files/txt_file_for_tagger.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c86358-c4c1-47f5-a7c4-c8b205416ffa",
   "metadata": {},
   "source": [
    "Next, run the following lines in your command line inside the conda environment, replacing \"rnntagger_path\" with the absolute path to your RNNTagger directory, \"txt_file_for_tagger.txt\" with the absolute path to said file (created in the cell above) and \"output\" with the absolute path leading to a new file called \"VACC/2_Preprocessed/Files/RNN_tagged.txt\" (i.e., complement that path based on where you stored the entire repository on your computer). Execute the middle line only if permission errors occur. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb2e6ea8-a447-4f2e-ba4b-b6d53c33b0fe",
   "metadata": {},
   "source": [
    "cd rnntagger_path\n",
    "chmod -R +x . \n",
    "cmd/rnn-tagger-german.sh txt_file_for_tagger.txt > output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8729c47a-d746-4f31-8356-392d3ff4e307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remapping tagged tokens to their respective turn\n",
    "preprocessing.remap(f\"1_Corpus/Corpus_merged_turns_{which_corpus}.csv\", #corpus\n",
    "                    f\"2_Preprocessed/Files/RNN_tagged.txt\", #tagger output\n",
    "                    tokens_for_remapping, #needed for remapping tagger output to corpus\n",
    "                    f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", #destination of remapped corpus\n",
    "                    which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fef32d7-3610-4f9d-b4e2-694062768559",
   "metadata": {},
   "source": [
    "Note that, as explained in the thesis, manual lemma correction was performed at this point.\n",
    "\n",
    "### Creating lemma bi-, tri-, and quadrigrams\n",
    "\n",
    "The following code creates lemma n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d96fc-fb9e-410f-88e5-295dde47a6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.ngrammer(f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414814c4-ac0c-4bf2-a8aa-30aebe2bc169",
   "metadata": {},
   "source": [
    "Preprocessing of the corpus is now done. Note again that even if you ran the code with the actual data, it will to some extent be different from the data used in the thesis due to non-replicable manual steps (see above). \n",
    "\n",
    "For the quantitative analysis see separate notebooks in the directory \"Quantitative_Analysis\". This code is in separate notebooks as some quantitative analyses extend beyond one single corpus and are thus organised with regard to alternation sets. Also, a different programming language (R) is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ed2dd3-5400-4ece-bdf6-91804d6df5b6",
   "metadata": {},
   "source": [
    "## Persistence Tagging for the Qualitative Analysis\n",
    "\n",
    "As mentioned in the thesis, the persistence tagging algorithm can be used to tag cases of persistence on multiple levels such as lemmata, POS-tags etc. However, the qualitative analysis in the thesis relied solely on lemma-based tagging. This is defined in the following cell, along with stop lemmata and lemmata from the instructions, both of which will be excluded from tagging. For simplicity, the actual instructions are excluded even when using dummy data.\n",
    "\n",
    "First, \"real\" cases of persistence from the voice assistant to the human speaker are tagged which the qualitative analysis relied on. Subsequently, instances of quasi-persistence from the human speaker to the voice assistant are tagged which were used as a variable in the model for the DEZEMBER alternation. For that, default values for \"speaker_A\" and \"speaker_B\" (see module code) are overwritten.\n",
    "\n",
    "### Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e99bce-074a-4bdb-9c3a-43f5c0603b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [\"lemma\"] #defining level to tag on \n",
    "\n",
    "stoplemmas = ['an', 'der', 'ein', 'es', 'f√ºr', 'haben', 'ich', 'in', 'mit', \n",
    "              'nicht', 'oder', 'sein', 'um', 'und', 'von', 'werden', 'zu'] #defining stopwords to exclude from tagging\n",
    "\n",
    "#reading and defining instructions for Quiz interactions w/o confederate as well as Calendar interactions\n",
    "with open(\"Instructions/Lemmata_in_instructions_with_confederate.txt\") as f, open(\"Instructions/Lemmata_in_instructions_without_confederate.txt\") as g, open(\"Instructions/Lemmata_in_visual_schedule.txt\") as h:\n",
    "    instructions_with_jannik, instructions_without_jannik, schedule = f.read().split(), g.read().split(), h.read().split()\n",
    "\n",
    "instructions = [instructions_without_jannik, instructions_with_jannik, schedule] #creating list of instructions to pass below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e10bacb-d97e-41cf-9708-e0f95e364c7a",
   "metadata": {},
   "source": [
    "### Tagging\n",
    "\n",
    "#### From voice assistant to human speaker \n",
    "\n",
    "##### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f9932-b857-4416-9cf5-008b271318b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading unigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing unigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_unigrams.csv\", instructions, stoplemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bcd00-1a52-4d04-b302-9bf7094177dd",
   "metadata": {},
   "source": [
    "##### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b295f8-bf01-4595-8f02-b67b4b001c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_bigrams = [[[\" \".join(instructions_without_jannik[i:i+2]) for i in range(len(instructions_without_jannik)-2+1)]],\n",
    "                       [[\" \".join(instructions_with_jannik[i:i+2]) for i in range(len(instructions_with_jannik)-2+1)]], schedule]\n",
    "\n",
    "#reading bigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_bigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing bigram corpus to persistence tagger while specifying levels, output destination and instructions \n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_bigrams.csv\", instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8767b52d-488b-48b5-bec2-5e03af9ac1cc",
   "metadata": {},
   "source": [
    "##### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534e30c5-fe5b-4315-b8f0-55525a5ba8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating trigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_trigrams = [[[\" \".join(instructions_without_jannik[i:i+3]) for i in range(len(instructions_without_jannik)-3+1)]],\n",
    "                        [[\" \".join(instructions_with_jannik[i:i+3]) for i in range(len(instructions_with_jannik)-3+1)]], schedule]\n",
    "\n",
    "#reading trigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_trigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing trigram corpus to persistence tagger while specifying levels, output destination and instructions \n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_trigrams.csv\", instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c8c8c-9fca-4979-9e5f-65dfe687600f",
   "metadata": {},
   "source": [
    "##### Quadrigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f3af43-dc62-4a60-b9ee-335c47642724",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating quadrigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_quadrigrams = [[[\" \".join(instructions_without_jannik[i:i+4]) for i in range(len(instructions_without_jannik)-4+1)]],\n",
    "                           [[\" \".join(instructions_with_jannik[i:i+4]) for i in range(len(instructions_with_jannik)-4+1)]], schedule]\n",
    "\n",
    "#reading quadrigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_quadrigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing quadrigram corpus to persistence tagger while specifying levels, output destination and instructions \n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Persistence_{which_corpus}_quadrigrams.csv\", instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82a4bc-5558-4b58-a51a-0d2068cdbc35",
   "metadata": {},
   "source": [
    "For simplicity, the following code combines all n-gram levels into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc18dda-aefe-4686-82da-7f5b86f33ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence.combiner(\"3_Persistence_tagged/single_ngrams\", f\"3_Persistence_tagged/Persistence_{which_corpus}_all.csv\", which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e2fab-1937-466f-8370-e30a7c985d91",
   "metadata": {},
   "source": [
    "#### From human speaker to voice assistant (quasi-persistence)\n",
    "\n",
    "##### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9424ad9a-f9f0-463d-ae2c-eaafe65b7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading unigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_unigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing unigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas,\n",
    "#and, crucially, switching direction (from voice assistant to human speaker) by overwriting default values\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Quasi_persistence_{which_corpus}_unigrams.csv\", instructions, stoplemmas,\n",
    "                  speaker_A=\"S\", speaker_B=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606ee8a-c9ee-48df-a2f1-4e3610db30df",
   "metadata": {},
   "source": [
    "##### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b50212-1967-42bc-bbd6-b5f870285bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating bigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_bigrams = [[[\" \".join(instructions_without_jannik[i:i+2]) for i in range(len(instructions_without_jannik)-2+1)]],\n",
    "                       [[\" \".join(instructions_with_jannik[i:i+2]) for i in range(len(instructions_with_jannik)-2+1)]], schedule]\n",
    "\n",
    "#reading bigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_bigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing unigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas,\n",
    "#and, crucially, switching direction (from voice assistant to human speaker) by overwriting default values\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Quasi_persistence_{which_corpus}_bigrams.csv\", instructions,\n",
    "                  speaker_A=\"S\", speaker_B=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d249b-4dad-4c00-a435-0f908715deef",
   "metadata": {},
   "source": [
    "##### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88d23d5-8ba2-4b30-a391-e74da8746065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating trigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_trigrams = [[[\" \".join(instructions_without_jannik[i:i+3]) for i in range(len(instructions_without_jannik)-3+1)]],\n",
    "                        [[\" \".join(instructions_with_jannik[i:i+3]) for i in range(len(instructions_with_jannik)-3+1)]], schedule]\n",
    "\n",
    "#reading trigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_trigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing unigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas,\n",
    "#and, crucially, switching direction (from voice assistant to human speaker) by overwriting default values\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Quasi_persistence_{which_corpus}_trigrams.csv\", instructions,\n",
    "                  speaker_A=\"S\", speaker_B=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72406053-6334-4e1a-866d-454a95386058",
   "metadata": {},
   "source": [
    "##### Quadrigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd48689-fdf8-489e-b189-a95e59438894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating quadrigrams from instructions (disregarding question boundaries for simplicity's sake; 'schedule' contains non-ordered lemmata, thus no n-grams are created)\n",
    "instructions_quadrigrams = [[[\" \".join(instructions_without_jannik[i:i+4]) for i in range(len(instructions_without_jannik)-4+1)]],\n",
    "                           [[\" \".join(instructions_with_jannik[i:i+4]) for i in range(len(instructions_with_jannik)-4+1)]], schedule]\n",
    "\n",
    "#reading quadrigram corpus\n",
    "corpus = pd.read_csv(f\"2_Preprocessed/RNN_{which_corpus}_quadrigrams.csv\", sep=\",\", index_col=0, na_filter=False)\n",
    "\n",
    "#passing unigram corpus to persistence tagger while specifying levels, output destination, instructions and stoplemmas,\n",
    "#and, crucially, switching direction (from voice assistant to human speaker) by overwriting default values\n",
    "persistence.tagger(corpus, which_corpus, levels, f\"3_Persistence_tagged/single_ngrams/Quasi_persistence_{which_corpus}_quadrigrams.csv\", instructions,\n",
    "                  speaker_A=\"S\", speaker_B=\"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4554d-2a7d-4978-8868-45a2809a3c46",
   "metadata": {},
   "source": [
    "For simplicity, the following code combines all n-gram levels into one DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714cde8-ca5d-4dba-b8a1-87b7b6a6c8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "persistence.combiner(\"3_Persistence_tagged/single_ngrams\", \"3_Persistence_tagged/Quasi_persistence_VACC_all.csv\", which_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610fd92-1730-494b-b741-f6b67b53587b",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "The following code visualises tagged cases of persistence on all n-grams levels in HTML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab5d9ce-3b60-45bc-b29b-09b220329cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation.lemma(which_corpus, \"3_Persistence_tagged\", \"3_Persistence_tagged/visualisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcbfbda-5499-41eb-8b24-b64e5ec73753",
   "metadata": {},
   "source": [
    "### Inspecting frequent cases of persistence\n",
    "\n",
    "The following code outputs frequent cases of persistence from the voice assistant to the human speaker for each n-gram level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e069a-8329-433d-a0b6-04bd22e7552b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualisation.inspect(levels = [\"lemma\"], #further levels such as POS-tags could be supplied if tagging was performed on that level\n",
    "                      ngrams = [\"unigrams\", \"bigrams\", \"trigrams\", \"quadrigrams\"], \n",
    "                      threshold = 0, \n",
    "                      which_corpus =  which_corpus, \n",
    "                      path = \"3_Persistence_tagged/single_ngrams\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvai",
   "language": "python",
   "name": "hvai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
